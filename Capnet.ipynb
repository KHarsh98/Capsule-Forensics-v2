{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Capnet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNwF8LH//yYY8LrI+BMlHIv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KHarsh98/Capsule-Forensics-v2/blob/master/Capnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "If9DV1OdICkY",
        "colab_type": "code",
        "outputId": "7f95ac3d-b7c6-4d16-a34f-900d5556db67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lKtwj9YHyNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import torch\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gL6NYYfgKatW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Copyright (c) 2019, National Institute of Informatics\n",
        "All rights reserved.\n",
        "Author: Huy H. Nguyen\n",
        "-----------------------------------------------------\n",
        "Script for Capsule-Forensics-v2 model\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "sys.setrecursionlimit(15000)\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Variable\n",
        "import torchvision.models as models\n",
        "\n",
        "NO_CAPS= 10 #Changed from 10 to 15 capsules\n",
        "\n",
        "class StatsNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StatsNet, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.data.shape[0], x.data.shape[1], x.data.shape[2]*x.data.shape[3])\n",
        "\n",
        "        mean = torch.mean(x, 2)\n",
        "        std = torch.std(x, 2)\n",
        "\n",
        "        return torch.stack((mean, std), dim=1)\n",
        "\n",
        "class View(nn.Module):\n",
        "    def __init__(self, *shape):\n",
        "        super(View, self).__init__()\n",
        "        self.shape = shape\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input.view(self.shape)\n",
        "\n",
        "\n",
        "class VggExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VggExtractor, self).__init__()\n",
        "\n",
        "        self.vgg_1 = self.Vgg(models.vgg19(pretrained=True), 0, 18)\n",
        "        self.vgg_1.eval()\n",
        "\n",
        "    def Vgg(self, vgg, begin, end):\n",
        "        features = nn.Sequential(*list(vgg.features.children())[begin:(end+1)])\n",
        "        return features\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.vgg_1(input)\n",
        "\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "\n",
        "        self.capsules = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(256, 64, kernel_size=3, stride=1, padding=1), #changed from 64 to 16 filters - REVERSED\n",
        "                nn.BatchNorm2d(64), #changed from 64 to 16 - REVERSED\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(64, 16, kernel_size=3, stride=1, padding=1), #changed from 64 to 16 neurons - REVERSED\n",
        "                nn.BatchNorm2d(16),\n",
        "                nn.ReLU(),\n",
        "                StatsNet(),\n",
        "\n",
        "                nn.Conv1d(2, 8, kernel_size=5, stride=2, padding=2),#changed from 8 to 4 neurons - EEVERSED\n",
        "                nn.BatchNorm1d(8), #changed from 8 to 4\n",
        "                nn.Conv1d(8, 1, kernel_size=3, stride=1, padding=1), #changed from 8 to 4 - REVERSED\n",
        "                nn.BatchNorm1d(1),\n",
        "                View(-1, 8),\n",
        "                )\n",
        "                for _ in range(NO_CAPS)]\n",
        "        )\n",
        "\n",
        "    def squash(self, tensor, dim):\n",
        "        squared_norm = (tensor ** 2).sum(dim=dim, keepdim=True)\n",
        "        scale = squared_norm / (1 + squared_norm)\n",
        "        return scale * tensor / (torch.sqrt(squared_norm))\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = [capsule(x.detach()) for capsule in self.capsules]\n",
        "        output = torch.stack(outputs, dim=-1)\n",
        "\n",
        "        return self.squash(output, dim=-1)\n",
        "\n",
        "class RoutingLayer(nn.Module):\n",
        "    def __init__(self, gpu_id, num_input_capsules, num_output_capsules, data_in, data_out, num_iterations):\n",
        "        super(RoutingLayer, self).__init__()\n",
        "\n",
        "        self.gpu_id = gpu_id\n",
        "        self.num_iterations = num_iterations\n",
        "        self.route_weights = nn.Parameter(torch.randn(num_output_capsules, num_input_capsules, data_out, data_in))\n",
        "\n",
        "\n",
        "    def squash(self, tensor, dim):\n",
        "        squared_norm = (tensor ** 2).sum(dim=dim, keepdim=True)\n",
        "        scale = squared_norm / (1 + squared_norm)\n",
        "        return scale * tensor / (torch.sqrt(squared_norm))\n",
        "\n",
        "    def forward(self, x, random, dropout):\n",
        "        # x[b, data, in_caps]\n",
        "\n",
        "        x = x.transpose(2, 1)\n",
        "        # x[b, in_caps, data]\n",
        "\n",
        "        if random:\n",
        "            noise = Variable(0.01*torch.randn(*self.route_weights.size()))\n",
        "            if self.gpu_id >= 0:\n",
        "                noise = noise.cuda(self.gpu_id)\n",
        "            route_weights = self.route_weights + noise\n",
        "        else:\n",
        "            route_weights = self.route_weights\n",
        "\n",
        "        priors = route_weights[:, None, :, :, :] @ x[None, :, :, :, None]\n",
        "\n",
        "        # route_weights [out_caps , 1 , in_caps , data_out , data_in]\n",
        "        # x             [   1     , b , in_caps , data_in ,    1    ]\n",
        "        # priors        [out_caps , b , in_caps , data_out,    1    ]\n",
        "\n",
        "        priors = priors.transpose(1, 0)\n",
        "        # priors[b, out_caps, in_caps, data_out, 1]\n",
        "\n",
        "        if dropout > 0.0:\n",
        "            drop = Variable(torch.FloatTensor(*priors.size()).bernoulli(1.0- dropout))\n",
        "            if self.gpu_id >= 0:\n",
        "                drop = drop.cuda(self.gpu_id)\n",
        "            priors = priors * drop\n",
        "            \n",
        "\n",
        "        logits = Variable(torch.zeros(*priors.size()))\n",
        "        # logits[b, out_caps, in_caps, data_out, 1]\n",
        "\n",
        "        if self.gpu_id >= 0:\n",
        "            logits = logits.cuda(self.gpu_id)\n",
        "\n",
        "        num_iterations = self.num_iterations\n",
        "\n",
        "        for i in range(num_iterations):\n",
        "            probs = F.softmax(logits, dim=2)\n",
        "            outputs = self.squash((probs * priors).sum(dim=2, keepdim=True), dim=3)\n",
        "\n",
        "            if i != self.num_iterations - 1:\n",
        "                delta_logits = priors * outputs\n",
        "                logits = logits + delta_logits\n",
        "\n",
        "        # outputs[b, out_caps, 1, data_out, 1]\n",
        "        outputs = outputs.squeeze()\n",
        "\n",
        "        if len(outputs.shape) == 3:\n",
        "            outputs = outputs.transpose(2, 1).contiguous() \n",
        "        else:\n",
        "            outputs = outputs.unsqueeze_(dim=0).transpose(2, 1).contiguous()\n",
        "        # outputs[b, data_out, out_caps]\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class CapsuleNet(nn.Module):\n",
        "    def __init__(self, num_class, gpu_id):\n",
        "        super(CapsuleNet, self).__init__()\n",
        "\n",
        "        self.num_class = num_class\n",
        "        self.fea_ext = FeatureExtractor()\n",
        "        self.fea_ext.apply(self.weights_init)\n",
        "\n",
        "        self.routing_stats = RoutingLayer(gpu_id=gpu_id, num_input_capsules=NO_CAPS, num_output_capsules=num_class, data_in=8, data_out=4, num_iterations=2)\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        classname = m.__class__.__name__\n",
        "        if classname.find('Conv') != -1:\n",
        "            m.weight.data.normal_(0.0, 0.02)\n",
        "        elif classname.find('BatchNorm') != -1:\n",
        "            m.weight.data.normal_(1.0, 0.02)\n",
        "            m.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, x, random=False, dropout=0.0):\n",
        "\n",
        "        z = self.fea_ext(x)\n",
        "        z = self.routing_stats(z, random, dropout=dropout)\n",
        "        # z[b, data, out_caps]\n",
        "\n",
        "        classes = F.softmax(z, dim=-1)\n",
        "\n",
        "        class_ = classes.detach()\n",
        "        class_ = class_.mean(dim=1)\n",
        "\n",
        "        return classes, class_\n",
        "\n",
        "class CapsuleLoss(nn.Module):\n",
        "    def __init__(self, gpu_id):\n",
        "        super(CapsuleLoss, self).__init__()\n",
        "        self.cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "        if gpu_id >= 0:\n",
        "            self.cross_entropy_loss.cuda(gpu_id)\n",
        "\n",
        "    def forward(self, classes, labels):\n",
        "        loss_t = self.cross_entropy_loss(classes[:,0,:], labels)\n",
        "\n",
        "        for i in range(classes.size(1) - 1):\n",
        "            loss_t = loss_t + self.cross_entropy_loss(classes[:,i+1,:], labels)\n",
        "\n",
        "        return loss_t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbezHeLZKgYH",
        "colab_type": "code",
        "outputId": "5f844739-25b5-481b-da40-9c889b029b74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import sys\n",
        "\n",
        "sys.setrecursionlimit(15000)\n",
        "import os\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "import argparse\n",
        "from sklearn import metrics\n",
        "from scipy.optimize import brentq\n",
        "from scipy.interpolate import interp1d\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "\n",
        "transform_fwd = transforms.Compose([\n",
        "        transforms.Resize((300, 300)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "    ])\n",
        "\n",
        "dataset_test = dset.ImageFolder(\"/content/gdrive/My Drive/FFHQ Dataset\", transform=transform_fwd)\n",
        "assert dataset_test\n",
        "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=32, shuffle=False,\n",
        "                                                  num_workers=0)\n",
        "\n",
        "vgg_ext = VggExtractor()\n",
        "capnet = CapsuleNet(2, 0)\n",
        "\n",
        "\n",
        "capnet.load_state_dict(torch.load(\"/content/capsule_21.pt\"))\n",
        "capnet.eval()\n",
        "\n",
        "vgg_ext.cuda(0)\n",
        "capnet.cuda(0)\n",
        "\n",
        "    ##################################################################################\n",
        "\n",
        "tol_label = np.array([], dtype=np.float)\n",
        "tol_pred = np.array([], dtype=np.float)\n",
        "tol_pred_prob = np.array([], dtype=np.float)\n",
        "\n",
        "\n",
        "count = 0\n",
        "loss_test = 0\n",
        "\n",
        "for img_data, labels_data in tqdm(dataloader_test):\n",
        "        \n",
        "    labels_data[labels_data > 0] = 5\n",
        "    labels_data[labels_data == 0] = 1\n",
        "    labels_data[labels_data == 5] = 0\n",
        "\n",
        "    img_label = labels_data.numpy().astype(np.float)\n",
        "    img_data = img_data.cuda(0)\n",
        "    labels_data = labels_data.cuda(0)\n",
        "\n",
        "    input_v = Variable(img_data)\n",
        "\n",
        "    x = vgg_ext(input_v)\n",
        "    classes, class_ = capnet(x, random=False)\n",
        "\n",
        "    output_dis = class_.data.cpu()\n",
        "    output_pred = np.zeros((output_dis.shape[0]), dtype=np.float)\n",
        "\n",
        "    for i in range(output_dis.shape[0]):\n",
        "        if output_dis[i, 1] >= output_dis[i, 0]:\n",
        "            output_pred[i] = 1.0\n",
        "        else:\n",
        "            output_pred[i] = 0.0\n",
        "\n",
        "    tol_label = np.concatenate((tol_label, img_label))\n",
        "    tol_pred = np.concatenate((tol_pred, output_pred))\n",
        "        \n",
        "    #pred_prob = torch.softmax(output_dis, dim=1)\n",
        "    #tol_pred_prob = np.concatenate((tol_pred_prob, pred_prob[:,1].data.numpy()))\n",
        "    count+= 1\n",
        "\n",
        "acc_test = metrics.accuracy_score(tol_label, tol_pred)\n",
        "\n",
        "loss_test /= count\n",
        "\n",
        "#fpr, tpr, thresholds = roc_curve(tol_label, tol_pred_prob, pos_label=1)\n",
        "#eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
        "\n",
        "#fnr = 1 - tpr\n",
        "#hter = (fpr + fnr)/2\n",
        "#print('[Epoch %d] Test acc: %.2f   EER: %.2f' % (0, acc_test * 100, eer * 100))\n",
        "    #text_writer.write('%d,%.2f,%.2f\\n' % (opt.id, acc_test * 100, eer * 100))\n",
        "\n",
        "print(acc_test * 100)\n",
        "print(tol_pred)  \n",
        "print(np.count_nonzero(tol_pred > 0.5))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [3:35:37<00:00, 20.70s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "47.925000000000004\n",
            "[0. 0. 0. ... 0. 0. 0.]\n",
            "4951\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0iEwDR_LEdQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}